{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install Newspaper3k"
      ],
      "metadata": {
        "id": "lTgUWufkuE4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from newspaper import Article\n",
        "\n",
        "df = pd.read_excel(\"/content/Input.xlsx\")\n",
        "urls = df[\"URL\"].tolist()\n",
        "url_ids = df[\"URL_ID\"].tolist()\n",
        "\n",
        "data = []\n",
        "for url, url_id in zip(urls, url_ids):\n",
        "    article = Article(url)\n",
        "\n",
        "    try:\n",
        "        article.download()\n",
        "        article.parse()\n",
        "\n",
        "        title = article.title\n",
        "        content = article.text\n",
        "\n",
        "        data.append({\"url_id\": url_id, \"url\": url, \"title\": title, \"content\": content})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing URL: {url}\\n{e}\")\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "RyeyHiGU4j0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['url_id'].to_string())"
      ],
      "metadata": {
        "id": "SrDnraTs4mco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in df.iterrows():\n",
        "    url_id = str(row[\"url_id\"])\n",
        "    title = row[\"title\"]\n",
        "    content = row[\"content\"]\n",
        "\n",
        "    # Creating the text file with title and content\n",
        "    with open(f\"{url_id}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"Title:\\n{title}\\n\\nContent:\\n{content}\")"
      ],
      "metadata": {
        "id": "3rQXFnr0G381"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"analysis\"] = df[\"title\"] + \"\\n\\n\" + df[\"content\"]\n"
      ],
      "metadata": {
        "id": "c7sx5KNsKvzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "id": "YlNy7O8ELH_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Loading stop words from NLTK\n",
        "stop_words = stopwords.words(\"english\")\n",
        "\n",
        "# Loading the stop words\n",
        "with open(\"/content/StopWords.txt\", \"r\") as f:\n",
        "    custom_stop_words = set(f.read().splitlines())\n",
        "    stop_words.extend(custom_stop_words)"
      ],
      "metadata": {
        "id": "qA6XfdQqn6Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to clean text\n",
        "def clean_text(text):\n",
        "    cleaned_text = []\n",
        "    for word in text.lower().split():\n",
        "        if word not in stop_words:\n",
        "            cleaned_text.append(word)\n",
        "    return \" \".join(cleaned_text)\n",
        "\n",
        "# Applying the cleaning function to the 'analysis' column\n",
        "df['analysis'] = df['analysis'].apply(clean_text)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "QZkcj9AxoWXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "punctuation = string.punctuation + \"â€™\"\n",
        "\n",
        "# Function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', punctuation))\n",
        "\n",
        "# Apply the function\n",
        "df['analysis'] = df['analysis'].apply(remove_punctuation)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "S4n1rhxlpHFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "\n",
        "# Loading positive and negative dictionaries\n",
        "positive_words = set(nltk.corpus.words.words(\"/content/positive_words.txt\"))\n",
        "with open(\"/content/negative_words.txt\", encoding=\"utf-8\") as f:\n",
        "    negative_words = set(f.read().splitlines())\n",
        "\n",
        "\n",
        "# Function to calculate sentiment scores\n",
        "def calculate_sentiment_scores(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    positive_score = sum(1 for token in tokens if token in positive_words)\n",
        "    negative_score = -1 * sum(1 for token in tokens if token in negative_words)\n",
        "    return positive_score, negative_score\n",
        "\n",
        "# Applying the function to create new columns\n",
        "results = df['analysis'].apply(calculate_sentiment_scores)\n",
        "df[['positive_score', 'negative_score']] = pd.DataFrame(results.tolist(), index=df.index)\n",
        "\n",
        "\n",
        "print(df)\n",
        "\n"
      ],
      "metadata": {
        "id": "VZctyfbTpLBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the total number of words in each row\n",
        "df[\"total_words\"] = df[\"analysis\"].apply(lambda text: len(text.split()))\n",
        "\n",
        "\n",
        "print(df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "acJwjvCk_yId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "bv7JeorMI4qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['positive_score', 'negative_score', 'total_words']].shape)\n"
      ],
      "metadata": {
        "id": "vG849hkMJBqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate polarity_score\n",
        "def calculate_polarity_score(positive_score, negative_score):\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)  # Avoid division by zero\n",
        "    return polarity_score\n",
        "\n",
        "# Creating the polarity_score column\n",
        "df['polarity_score'] = df[['positive_score', 'negative_score']].apply(\n",
        "    lambda row: calculate_polarity_score(row['positive_score'], row['negative_score']),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "dGw5MTeKJy8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['negative_score'] = df['negative_score'].abs()\n",
        "print(df)"
      ],
      "metadata": {
        "id": "Hdd9bm-CMe4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate polarity_score\n",
        "def calculate_polarity_score(positive_score, negative_score):\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)  # Avoid division by zero\n",
        "    return polarity_score\n",
        "\n",
        "# Creating the polarity_score column\n",
        "df['polarity_score'] = df[['positive_score', 'negative_score']].apply(\n",
        "    lambda row: calculate_polarity_score(row['positive_score'], row['negative_score']),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "dt2KAKFdMlw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate subjectivity_score with total_words\n",
        "def calculate_subjectivity_score(positive_score, negative_score, total_words):\n",
        "    subjectivity_score = (positive_score + negative_score) / (total_words + 0.000001)\n",
        "    return subjectivity_score\n",
        "\n",
        "# Creating the subjectivity_score column using both scores and total_words\n",
        "df['subjectivity_score'] = df[['positive_score', 'negative_score', 'total_words']].apply(\n",
        "    lambda row: calculate_subjectivity_score(row['positive_score'], row['negative_score'], row['total_words']),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "YgbClNy5NaYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate average sentence length\n",
        "def calculate_average_sentence_length(text):\n",
        "    words = text.split()\n",
        "    sentences = text.split('.')  # Assuming sentences end with periods\n",
        "    return len(words) / len(sentences)\n",
        "\n",
        "# Function to calculate average number of words per sentence\n",
        "def calculate_average_words_per_sentence(total_words, total_sentences):\n",
        "    return total_words / total_sentences\n",
        "\n",
        "# Applying both functions to create new columns\n",
        "df['average_sentence_length'] = df['content'].apply(calculate_average_sentence_length)\n",
        "df['average_words_per_sentence'] = calculate_average_words_per_sentence(df['total_words'].sum(), len(df))\n",
        "\n",
        "print(df)\n",
        "\n"
      ],
      "metadata": {
        "id": "UPRFmLSrrrE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import cmudict\n",
        "\n",
        "# Function to count complex words\n",
        "def count_complex_words(text):\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('cmudict')\n",
        "    d = cmudict.dict()\n",
        "    words = text.split()\n",
        "    complex_words = 0\n",
        "    for word in words:\n",
        "        # Using cmudict syllabification, handling potential errors\n",
        "        try:\n",
        "            syllables = d[word.lower()][0]\n",
        "            if len(syllables) > 2:\n",
        "                complex_words += 1\n",
        "        except (KeyError, IndexError):\n",
        "            pass\n",
        "    return complex_words\n",
        "\n",
        "# Applying the function and addding the complex_words column\n",
        "df['complex_words'] = df['analysis'].apply(count_complex_words)\n",
        "\n",
        "\n",
        "print(df)\n",
        "\n"
      ],
      "metadata": {
        "id": "T5O_pOKer1aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['complex_words_percentage'] = df['complex_words'] / df['total_words']\n"
      ],
      "metadata": {
        "id": "hS0H8PeyvNM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating and addding fog_index column\n",
        "df['fog_index'] = 0.4 * (df['average_sentence_length'] + df['complex_words_percentage'])\n",
        "\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "d6q4pf9bvS3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def count_syllables(word):\n",
        "  \"\"\"\n",
        "  This function counts the number of syllables in a word, handling exceptions\n",
        "  for \"es\" and \"ed\" endings.\n",
        "  \"\"\"\n",
        "  vowels = \"aeiou\"\n",
        "  num_vowels = 0\n",
        "  # Handling exceptions for \"es\" and \"ed\"\n",
        "  if word.endswith(\"es\") and len(word) > 2 and word[-2] not in vowels:\n",
        "    num_vowels -= 1\n",
        "  if word.endswith(\"ed\") and len(word) > 3 and word[-3] not in vowels:\n",
        "    num_vowels -= 1\n",
        "  # Counting vowels considering exceptions\n",
        "  for char in word:\n",
        "    if char.lower() in vowels:\n",
        "      num_vowels += 1\n",
        "  # Syllable count based on vowel count\n",
        "  if num_vowels == 0:\n",
        "    return 1\n",
        "  else:\n",
        "    return num_vowels\n",
        "\n",
        "# Applying the function to each word and summing the syllables\n",
        "df['total_syllables'] = df['analysis'].apply(lambda text: sum(count_syllables(word) for word in text.split()))\n",
        "\n",
        "\n",
        "print(df)\n",
        "\n"
      ],
      "metadata": {
        "id": "jBYlKOfSvqd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
        "pattern = r\"(?<!\\w)({})\\b\".format(\"|\".join(pronouns))\n",
        "\n",
        "# Applying regex and count occurrences\n",
        "df['personal_pronouns'] = df['analysis'].apply(lambda text: len(re.findall(pattern, text, re.IGNORECASE)))\n",
        "\n",
        "# Print or save the updated DataFrame\n",
        "print(df)"
      ],
      "metadata": {
        "id": "3LOdvi0Ayn_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating average word length based on character count in words\n",
        "df['average_word_length'] = df['analysis'].apply(\n",
        "    lambda text: sum(len(word) for word in text.split())\n",
        ") / df['total_words']\n",
        "\n",
        "\n",
        "print(df)\n",
        "\n"
      ],
      "metadata": {
        "id": "s0pE7t_j1YSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0npAZT4E1yPk",
        "outputId": "1c26d1bd-68cf-4e09-cd88-1164cfdd26d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['url_id',\n",
              " 'url',\n",
              " 'title',\n",
              " 'content',\n",
              " 'analysis',\n",
              " 'positive_score',\n",
              " 'negative_score',\n",
              " 'total_words',\n",
              " 'polarity_score',\n",
              " 'subjectivity_score',\n",
              " 'average_sentence_length',\n",
              " 'average_words_per_sentence',\n",
              " 'complex_words',\n",
              " 'complex_words_percentage',\n",
              " 'fog_index',\n",
              " 'total_syllables',\n",
              " 'personal_pronouns',\n",
              " 'average_word_length',\n",
              " 'total_chars']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_column_names = {\n",
        "    \"url_id\": \"URL_ID\",\n",
        "    \"url\": \"URL\",\n",
        "    \"positive_score\": \"POSITIVE SCORE\",\n",
        "    \"negative_score\": \"NEGATIVE SCORE\",\n",
        "    \"polarity_score\": \"POLARITY SCORE\",\n",
        "    \"subjectivity_score\": \"SUBJECTIVITY SCORE\",\n",
        "    \"average_sentence_length\": \"AVG SENTENCE LENGTH\",\n",
        "    \"complex_words_percentage\": \"PERCENTAGE OF COMPLEX WORDS\",\n",
        "    \"fog_index\": \"FOG INDEX\",\n",
        "    \"average_words_per_sentence\": \"AVG NUMBER OF WORDS PER SENTENCE\",\n",
        "    \"complex_words\": \"COMPLEX WORD COUNT\",\n",
        "    \"total_words\": \"WORD COUNT\",\n",
        "    \"total_syllables\": \"SYLLABLE PER WORD\",\n",
        "    \"personal_pronouns\": \"PERSONAL PRONOUNS\",\n",
        "    \"average_word_length\": \"AVG WORD LENGTH\",\n",
        "\n",
        "}\n",
        "\n",
        "df.rename(columns=new_column_names, inplace=True)"
      ],
      "metadata": {
        "id": "aWKSbaPNJDI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "id": "YVUxYbmpQ-ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "column_order = [\n",
        "    \"URL_ID\",\n",
        "    \"URL\",\n",
        "    \"POSITIVE SCORE\",\n",
        "    \"NEGATIVE SCORE\",\n",
        "    \"POLARITY SCORE\",\n",
        "    \"SUBJECTIVITY SCORE\",\n",
        "    \"AVG SENTENCE LENGTH\",\n",
        "    \"PERCENTAGE OF COMPLEX WORDS\",\n",
        "    \"FOG INDEX\",\n",
        "    \"AVG NUMBER OF WORDS PER SENTENCE\",\n",
        "    \"COMPLEX WORD COUNT\",\n",
        "    \"WORD COUNT\",\n",
        "    \"SYLLABLE PER WORD\",\n",
        "    \"PERSONAL PRONOUNS\",\n",
        "    \"AVG WORD LENGTH\",\n",
        "]\n",
        "\n",
        "# Selecting and export in the desired order, excluding unwanted columns\n",
        "dataframe_to_export = df[\n",
        "    [col for col in column_order if col not in [\"title\", \"content\", \"analysis\", \"total_chars\"]]\n",
        "]\n",
        "dataframe_to_export.to_excel(\"output.xlsx\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "L7ltYy1XRINu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6s2DIXAJRupi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YgbrJqWZS_TQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}